"""AI-powered content generation module.

This module provides LLM integration for generating content drafts,
SEO metadata, alt-text, and summaries. It follows a provider pattern
to support multiple LLM backends without framework dependencies.
"""

from typing import Dict, List, Optional, Any, Protocol
from dataclasses import dataclass
from abc import ABC, abstractmethod
import re


class LLMProvider(Protocol):
    """Protocol for LLM providers.
    
    This allows different LLM backends (OpenAI, Anthropic, local models, etc.)
    to be plugged in without tight coupling.
    """

    async def generate(
        self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7
    ) -> str:
        """Generate text from a prompt.
        
        Args:
            prompt: The input prompt
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            
        Returns:
            Generated text
        """
        ...


@dataclass
class ContentDraft:
    """Represents an AI-generated content draft."""

    title: str
    body: str
    excerpt: str
    suggested_tags: List[str]
    confidence: float  # 0-1 score of generation quality


@dataclass
class SEOMetadata:
    """SEO metadata generated by AI."""

    title: str
    description: str
    keywords: List[str]
    og_title: Optional[str] = None
    og_description: Optional[str] = None
    structured_data: Optional[Dict[str, Any]] = None


@dataclass
class AltTextSuggestion:
    """Alt text suggestion for an image."""

    alt_text: str
    long_description: Optional[str] = None
    confidence: float = 0.0


class ContentGenerator:
    """AI-powered content generator.
    
    This class provides methods for generating various types of content
    using LLM providers. It can work with or without an actual LLM backend,
    falling back to rule-based generation when needed.
    """

    def __init__(self, llm_provider: Optional[LLMProvider] = None) -> None:
        """Initialize the content generator.
        
        Args:
            llm_provider: Optional LLM provider for AI generation
        """
        self.llm_provider = llm_provider

    async def generate_draft(
        self,
        topic: str,
        content_type: str = "article",
        keywords: Optional[List[str]] = None,
        tone: str = "professional",
        length: str = "medium",
    ) -> ContentDraft:
        """Generate a content draft.
        
        Args:
            topic: The topic to write about
            content_type: Type of content (article, blog, guide, etc.)
            keywords: Optional keywords to include
            tone: Tone of the content (professional, casual, technical, etc.)
            length: Length preference (short, medium, long)
            
        Returns:
            ContentDraft with generated content
        """
        if self.llm_provider:
            return await self._generate_draft_with_llm(
                topic, content_type, keywords, tone, length
            )
        return self._generate_draft_fallback(topic, content_type, keywords)

    async def _generate_draft_with_llm(
        self,
        topic: str,
        content_type: str,
        keywords: Optional[List[str]],
        tone: str,
        length: str,
    ) -> ContentDraft:
        """Generate draft using LLM provider."""
        keywords_str = ", ".join(keywords) if keywords else ""
        
        length_guidance = {
            "short": "around 300-500 words",
            "medium": "around 800-1200 words",
            "long": "around 1500-2500 words",
        }
        
        prompt = f"""Generate a {tone} {content_type} about: {topic}

Requirements:
- Length: {length_guidance.get(length, 'medium length')}
- Tone: {tone}
{f'- Include these keywords naturally: {keywords_str}' if keywords_str else ''}

Format the response as:
TITLE: [title]
EXCERPT: [brief excerpt]
TAGS: [comma-separated tags]
BODY:
[main content]
"""

        response = await self.llm_provider.generate(
            prompt, max_tokens=2000, temperature=0.7
        )
        
        return self._parse_draft_response(response)

    def _generate_draft_fallback(
        self, topic: str, content_type: str, keywords: Optional[List[str]]
    ) -> ContentDraft:
        """Generate draft using rule-based approach when LLM is not available."""
        # Extract key phrases from topic
        title = f"{content_type.title()}: {topic}"
        
        # Generate simple body
        body = f"""# {topic}

## Introduction

This {content_type} explores {topic.lower()}. """
        
        if keywords:
            body += f"Key topics include {', '.join(keywords)}. "
        
        body += f"""

## Main Content

[Content about {topic} goes here]

## Key Points

- Important aspect of {topic}
- Another consideration
- Final thoughts

## Conclusion

In summary, {topic.lower()} is an important topic that deserves careful consideration.
"""
        
        # Generate excerpt
        excerpt = f"An exploration of {topic.lower()}"
        if keywords:
            excerpt += f", covering {keywords[0]}"
        excerpt += "."
        
        # Suggest tags
        suggested_tags = keywords if keywords else [topic.lower()]
        
        return ContentDraft(
            title=title,
            body=body,
            excerpt=excerpt,
            suggested_tags=suggested_tags,
            confidence=0.6,  # Lower confidence for fallback
        )

    def _parse_draft_response(self, response: str) -> ContentDraft:
        """Parse LLM response into ContentDraft."""
        title = ""
        excerpt = ""
        tags = []
        body = ""
        
        # Parse structured response
        lines = response.strip().split("\n")
        current_section = None
        
        for line in lines:
            if line.startswith("TITLE:"):
                title = line[6:].strip()
            elif line.startswith("EXCERPT:"):
                excerpt = line[8:].strip()
            elif line.startswith("TAGS:"):
                tags = [t.strip() for t in line[5:].split(",")]
            elif line.startswith("BODY:"):
                current_section = "body"
            elif current_section == "body":
                body += line + "\n"
        
        # Fallback parsing if structured format not found
        if not title:
            title = response.split("\n")[0][:100]
        if not body:
            body = response
        
        return ContentDraft(
            title=title,
            body=body.strip(),
            excerpt=excerpt or body[:200] + "...",
            suggested_tags=tags or [],
            confidence=0.8,
        )

    async def generate_seo_metadata(
        self, content: str, target_keywords: Optional[List[str]] = None
    ) -> SEOMetadata:
        """Generate SEO metadata for content.
        
        Args:
            content: The content to generate metadata for
            target_keywords: Optional target keywords
            
        Returns:
            SEOMetadata with generated fields
        """
        if self.llm_provider:
            return await self._generate_seo_with_llm(content, target_keywords)
        return self._generate_seo_fallback(content, target_keywords)

    async def _generate_seo_with_llm(
        self, content: str, target_keywords: Optional[List[str]]
    ) -> SEOMetadata:
        """Generate SEO metadata using LLM."""
        keywords_str = ", ".join(target_keywords) if target_keywords else ""
        
        # Truncate content if too long
        content_preview = content[:1000] if len(content) > 1000 else content
        
        prompt = f"""Generate SEO metadata for this content:

{content_preview}

{f'Target keywords: {keywords_str}' if keywords_str else ''}

Provide:
1. SEO Title (50-60 characters)
2. Meta Description (150-160 characters)
3. Keywords (5-10 relevant keywords)
4. Open Graph Title
5. Open Graph Description

Format:
SEO_TITLE: [title]
META_DESC: [description]
KEYWORDS: [keyword1, keyword2, ...]
OG_TITLE: [title]
OG_DESC: [description]
"""

        response = await self.llm_provider.generate(
            prompt, max_tokens=500, temperature=0.5
        )
        
        return self._parse_seo_response(response)

    def _generate_seo_fallback(
        self, content: str, target_keywords: Optional[List[str]]
    ) -> SEOMetadata:
        """Generate SEO metadata using rule-based approach."""
        # Extract first heading or first sentence as title
        lines = content.split("\n")
        title = ""
        for line in lines:
            if line.startswith("#"):
                title = line.lstrip("#").strip()
                break
        
        if not title:
            # Use first sentence
            sentences = re.split(r"[.!?]", content)
            title = sentences[0][:60] if sentences else "Content"
        
        # Ensure title is within SEO limits
        if len(title) > 60:
            title = title[:57] + "..."
        
        # Generate description from first paragraph
        paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]
        description = paragraphs[0][:160] if paragraphs else content[:160]
        if len(description) == 160 and len(content) > 160:
            description = description[:157] + "..."
        
        # Extract keywords
        keywords = target_keywords if target_keywords else []
        if not keywords:
            # Simple keyword extraction
            words = re.findall(r"\b[a-z]{4,}\b", content.lower())
            word_freq = {}
            for word in words:
                if word not in ["that", "this", "with", "from", "have", "been"]:
                    word_freq[word] = word_freq.get(word, 0) + 1
            keywords = sorted(word_freq, key=word_freq.get, reverse=True)[:7]
        
        return SEOMetadata(
            title=title,
            description=description,
            keywords=keywords,
            og_title=title,
            og_description=description,
        )

    def _parse_seo_response(self, response: str) -> SEOMetadata:
        """Parse LLM SEO response."""
        title = ""
        description = ""
        keywords = []
        og_title = None
        og_description = None
        
        lines = response.strip().split("\n")
        for line in lines:
            if line.startswith("SEO_TITLE:"):
                title = line[10:].strip()
            elif line.startswith("META_DESC:"):
                description = line[10:].strip()
            elif line.startswith("KEYWORDS:"):
                keywords = [k.strip() for k in line[9:].split(",")]
            elif line.startswith("OG_TITLE:"):
                og_title = line[9:].strip()
            elif line.startswith("OG_DESC:"):
                og_description = line[8:].strip()
        
        return SEOMetadata(
            title=title or "Untitled",
            description=description,
            keywords=keywords,
            og_title=og_title,
            og_description=og_description,
        )

    async def generate_alt_text(
        self, image_path: str, context: Optional[str] = None
    ) -> AltTextSuggestion:
        """Generate alt text for an image.
        
        Args:
            image_path: Path to the image file
            context: Optional context about the image
            
        Returns:
            AltTextSuggestion with generated alt text
        """
        if self.llm_provider and hasattr(self.llm_provider, "analyze_image"):
            # If provider supports image analysis
            return await self._generate_alt_text_with_vision(image_path, context)
        return self._generate_alt_text_fallback(image_path, context)

    def _generate_alt_text_fallback(
        self, image_path: str, context: Optional[str]
    ) -> AltTextSuggestion:
        """Generate alt text using filename and context."""
        import os
        
        # Extract information from filename
        filename = os.path.basename(image_path)
        name_parts = os.path.splitext(filename)[0].replace("-", " ").replace("_", " ")
        
        alt_text = f"Image: {name_parts}"
        if context:
            alt_text = f"{name_parts} - {context}"
        
        # Clean up and capitalize
        alt_text = " ".join(alt_text.split())
        alt_text = alt_text[0].upper() + alt_text[1:] if alt_text else "Image"
        
        return AltTextSuggestion(
            alt_text=alt_text[:125],  # Keep within reasonable length
            confidence=0.5,  # Lower confidence for fallback
        )

    async def summarize_content(
        self, content: str, max_length: int = 200, style: str = "brief"
    ) -> str:
        """Generate a summary of content.
        
        Args:
            content: Content to summarize
            max_length: Maximum length of summary in characters
            style: Summary style (brief, detailed, bullet_points)
            
        Returns:
            Summary text
        """
        if self.llm_provider:
            return await self._summarize_with_llm(content, max_length, style)
        return self._summarize_fallback(content, max_length)

    async def _summarize_with_llm(
        self, content: str, max_length: int, style: str
    ) -> str:
        """Summarize using LLM."""
        prompt = f"""Summarize the following content in a {style} style, 
keeping it under {max_length} characters:

{content[:2000]}

Summary:"""

        response = await self.llm_provider.generate(
            prompt, max_tokens=int(max_length / 3), temperature=0.3
        )
        
        return response.strip()[:max_length]

    def _summarize_fallback(self, content: str, max_length: int) -> str:
        """Simple extractive summarization."""
        # Remove markdown and HTML
        cleaned = re.sub(r"<[^>]+>", "", content)
        cleaned = re.sub(r"[#*`]", "", cleaned)
        
        # Get first meaningful paragraph or sentences
        paragraphs = [p.strip() for p in cleaned.split("\n\n") if p.strip()]
        
        if paragraphs:
            summary = paragraphs[0]
        else:
            summary = cleaned
        
        # Truncate to max length
        if len(summary) > max_length:
            summary = summary[: max_length - 3] + "..."
        
        return summary
